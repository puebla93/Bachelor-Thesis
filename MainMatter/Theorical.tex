%===================================================================================
% Chapter: Theorical
%===================================================================================
\chapter{Marco Teórico}\label{chapter:theorical}
%===================================================================================

El objetivo principal de este trabajo es desarrollar un sistema que permita conocer en tiempo real si un lanzamiento de un pitcher de béisbol fue strike o bola utilizando una única cámara \textit{PlayStation Eye}. Es válido señalar que el sistema utilizará una única cámara, puesto que los sistemas existentes en su gran mayoría utilizan cámaras estereoscópicas. La información que brinda el sistema en cada lanzamiento consiste en definir si este fue strike o bola, así como su velocidad, pudiendo establecer con esta información varios datos estadísticos con respecto al comportamiento del pitcher luego de un conjunto de lanzamientos.

Para comprender la solución que se propone en este trabajo se enumeran las cuatro etapas en que se dividió el proceso de caracterización de un lanzamiento:

\begin{itemize}
    \item Obtención de los cinco puntos que describen el \textit{Home Plate}.
    \item Computar la matriz de tranformación de la imagen a una vista top-view.
    \item Detección de la pelota en la imagen.
    \item Modelación de la trayectoria de la pelota.
\end{itemize}

A lo largo de este capítulo se pretende describir el fundamento teórico y los principales algoritmos utilizados en cada etapa.

\section{Modelo de cámara}

Una cámara es un mapeo entre el mundo 3D (espacio de objetos) y una imagen 2D. Los modelos de cámara son matrices con propiedades particulares que representan el mapeo de la cámara. El modelo de cámara más especializado y simple es el modelo monocular sencillo o \textit{pinhole} \cite{RichardAndrew}.\\

\textit{\textbf{Pinhole.}} Sea $O$ centro de proyección y origen de un sistema de coordenadas Euclidiano, dado el plano $Z = f$, denominado \textit{plano de la imagen} o \textit{plano focal}, la proyección $q = (x,y,f)$ de todo punto $Q = (X,Y,Z)$ en el espacio, es la intersección entre el plano focal y la recta definida por $Q$ y $O$ (ver Fig. \ref{fig:PinholeModel}).

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{Graphics/PinholeModel.png}
    \caption{Modelo \textit{pinhole} \cite{DVD}.}
    \label{fig:PinholeModel}
\end{figure}

El centro de proyección $o$ se denomina \textit{centro óptico}. La distancia $f$ ente el centro óptico y el plano focal es la \textit{distancia focal}. La recta que pasa por el centro óptico y es perpendicular al plano focal se llama \textit{eje principal} de la cámara y el punto donde el eje principal intercepta el plano de la imagen se denomina \textit{punto principal}. El plano que pasa por el centro óptico y es paralelo al plano de la imagen se denomina \textit{plano principal} \cite{DVD}.

Dado que los pares de triángulos oad, obc y oqa, oQb son semejantes \cite{DVD}, se cumple que las
coordenadas de la proyección del punto Q en el plano de la imagen son:

\begin{equation}
    x=f\frac{X}{Z}, \quad y=f\frac{Y}{Z}
    \label{eq:ProjectionCoordinates}
\end{equation}

La expresión \ref{eq:ProjectionCoordinates} supone que el origen de las coordenadas en el plano de la imagen está en el eje principal \cite{RichardAndrew}. En la práctica, puede no serlo, por lo que es necesario introducir dos nuevos parámetros, $c_x$ y $c_y$, para modelar un posible desplazamiento del centro de coordenadas en la pantalla de proyección \cite{GaryAdrian}. Los píxeles en una cámara de bajo costo son rectangulares en lugar de cuadrados \cite{GaryAdrian}, por dicha razón se introducen dos nuevas distancias focales $f_x=fs_x$ y $f_y=fs_y$ donde $s_x/s_y$ representa la relación de aspecto de los pixeles \cite{DVD}. Teniendo en cuenta los nuevos parámetros y la expresión \ref{eq:ProjectionCoordinates}, el punto en el plano de la imagen se define como:

\begin{equation}
    x=f_x\bigg(\frac{X}{Z}\bigg) + c_x, \quad y=f_y\bigg(\frac{Y}{Z}\bigg) + c_y
    \label{eq:ProjectionCoordinates2}
\end{equation}

Si se utilizan coordenadas homogéneas \cite{JamesAndriesStevenJohnRichard} es posible representar la ecuación \ref{eq:ProjectionCoordinates2} como un producto de matrices:
$$
    q=MQ, \text{ donde} \quad
    q = \begin{bmatrix}x\\y\\w\end{bmatrix},\quad
    M = \begin{bmatrix}
	    f_x & 0 & c_x \\
	    0 & f_y & c_y \\
	    0 & 0 & 1
        \end{bmatrix},\quad
    Q = \begin{bmatrix}X\\Y\\Z\end{bmatrix}
$$

Esta representación permite agrupar los parámetros $f_x$, $f_y$, $c_x$ y $c_y$ de la cámara en una matriz $M_{3x3}$ denominada matriz de parámetros intrínsecos.

\subsection{Distorsiones del lente}

La distorsión del lente tiene lugar durante la proyección inicial del mundo en el plano de la imagen. En teoría, es posible definir un lente que no introduzca distorsiones. Sin embargo en la práctica, ningún lente es perfecto. Esto se debe principalmente a razones de fabricación, puesto que los lentes poseen una forma esférica y es difícil alinear exactamente el lente y la cámara de forma mecánica.\\

\textbf{Distorsión radial.}
Los lentes de las cámaras reales a menudo distorsionan la ubicación de los píxeles cerca de los bordes de la cámara. Este fenómeno abultado es la fuente del efecto ``barril'' u ``ojo de pez''. La figura \ref{fig:RadialDistortion} da alguna intuición sobre por qué ocurre esta distorsión.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{Graphics/RadialDistortion.png}
    \caption{Distorsión radial causada por el lente de la cámara \cite{DVD}.}
    \label{fig:RadialDistortion}
\end{figure}

Esta distorsión puede caracterizarse por una función $f(r)$, donde r es la distancia del punto distorsionado al centro óptico. Si aproximamos $f(r)$ por los primeros términos de una expansión de la serie de Taylor \cite{Nicolas} se tiene:.
$$f(r)=a_0+a_1r+a_2r^2+\dots$$

Dado que se asume que no hay distorsión en el centro de la imagen se tiene que $f(0)=0$ y por tanto $a_0=0$, como $f$ es simétrica en $r$, todos los coeficientes de las potencias impares son nulos. En general, la distorsión radial se modela utilizando las ecuaciones:
\begin{gather*}
    x_c = x(1 + k_1 r^2 + k_2 r^4 + k_3 r^6)\\
    y_c = y(1 + k_1 r^2 + k_2 r^4 + k_3 r^6)
\end{gather*}

Donde $(x, y)$ son las coordenadas originales, $k_1$, $k_2$ y $k_3$ son los términos de la serie de Taylor $a_2$, $a_4$ y $a_6$ respectivamente y $(x_c, y_c)$ es la nueva posición rectificada. En la mayoría de los casos no es necesario tener en cuenta el parámetro $k_3$ a excepción de las cámaras con lentes que provocan una deformación radial extrema \cite{DVD}.\\

\textbf{Distorsión tangencial.}
Otra de las distorsión más comunes es la distorsión tangencial. Esta distorsión se debe a defectos de fabricación, los cuales resultan en que el lente no está exactamente paralelo al plano de imagen (ver Fig. \ref{fig:TangentialDistortion}). La fórmula propuesta en \cite{GaryAdrian} para corregir esta distorsión es:
\begin{gather*}
    x_c = x + [2p_1y + p_2(r^2 + 2x^2)]\\
    y_c = y + [p_1(r^2 + 2y^2) + 2p_2x]
\end{gather*}

\begin{figure}[h!]
	\centering
	\includegraphics[width=\linewidth]{Graphics/TangentialDistortion.png}
	\caption{Distorsión tangencial \cite{DVD}.}
	\label{fig:TangentialDistortion}
\end{figure}

Hay muchos otros tipos de distorsiones que ocurren en los sistemas de imágenes, pero generalmente tienen efectos menores que las distorsiones radiales y tangenciales.

\section{Calibración}

El proceso de calibración de la cámara nos proporciona un modelo de la geometría de la cámara y un modelo de distorsión del lente. Estos dos modelos informativos definen los parámetros intrínsecos de la cámara. La calibración de la cámara también es importante para relacionar las mediciones de la cámara con mediciones en el mundo tridimensional real \cite{GaryAdrian}. Esto es importante porque las escenas no son solo tridimensionales; también son espacios físicos con unidades físicas. Según \cite{GaryAdrian} la calibración es: \textit{el proceso mediante el cual se calculan los parámetros intrínsecos y extrínsecos de la cámara, a partir de la correspondencia entre un conjunto de puntos de control con coordenadas tridimensionales conocidas y sus proyecciones en la imagen obtenida con dicha cámara}.

Para calcular los parámetros intrínsecos, $f_x$, $f_y$, $c_x$, $c_y$ y el vector de coeficientes de distorsión, se capturan imágenes de una escena que tenga un conjunto $C=(Q_1 \dots Q_n)$ de puntos característicos (fácilmente identificables) con coordenadas conocidas con respecto a un origen previamente definido. Cada punto $Q_i\in{C}$ tiene una proyección $q_i=(x, y)$ en el plano de la imagen con coordenadas en pixeles. Es posible describir la posición relativa de los puntos $Q$ con respecto a las coordenadas de la cámara (con origen en el punto principal) utilizando una rotación $R$ y una traslación $T$ de modo que $Q_c=R(Q_w-T)$, donde $Q_w$ son las coordenadas de los puntos en el espacio y $Q_c$ son las coordenadas con respecto a la cámara y las matrices $R$, $T$ se denominan parámetros extrínsecos de la cámara. Utilizando coordenadas homogéneas y la matriz de parámetros intrínsecos $M$ es posible definir la homografía $H$ que relaciona a cada punto del espacio $Q_i$ con sus proyecciones $q_i$ en el plano de la imagen \cite{DVD}
\begin{equation*}
    \begin{split}
        q'=sHQ',\quad\text{donde}\qquad &
        \begin{split}
            q' & = \begin{bmatrix}x&y&1\end{bmatrix}^T\\
            Q' & = \begin{bmatrix}X&Y&Z&1\end{bmatrix}^T\\
            H & = sMW\\
            M & = \begin{bmatrix}
	            f_x & 0 & c_x \\
	            0 & f_y & c_y \\
	            0 & 0 & 1
                \end{bmatrix}\\
            W & = \begin{bmatrix}R&T\end{bmatrix}
        \end{split}
    \end{split}
\end{equation*}

El parámetro $s$ es un factor de escalado arbitrario que hace explícito el hecho de que la homografía solo está definida hasta este factor y $W_{3x4}$ es una matriz donde las tres primeras columnas es la matriz de rotación $R$ y la última columna es el vector de traslación $T$. Resolver este sistema de ecuaciones resulta en la obtención de los parámetros intrínsecos y extrínsecos de la cámara \cite{DVD}.

\section{Eliminación de ruido}

La eliminación de ruido en imágenes es una tarea vital de procesamiento de imágenes, como un proceso en sí mismo o como un componente en otros procesos \cite{PawanManojSumitAshok}. En todos los sistemas de procesamiento debemos considerar qué parte de la señal detectada puede considerarse verdadera y qué tanto está asociada con los eventos de fondo aleatorios resultantes del proceso de detección o transmisión. Estos eventos aleatorios se clasifican bajo el tema general de ruido. Este ruido puede ser el resultado de una amplia variedad de fuentes, incluyendo la naturaleza discreta de la radiación, variación en la sensibilidad del detector, efectos de granos fotográficos, errores de transmisión de datos, propiedades de sistemas de imágenes como turbulencia de aire o gotas de agua y errores de cuantificación de imágenes. En cada caso, las propiedades del ruido son diferentes, al igual que las operaciones de procesamiento de imágenes que se pueden aplicar para reducir sus efectos \cite{topic5}. En esta sección, bindaremos una breve descripción de varios modelos de ruido \cite{AjayBrijendra} y los diferentes tipos de filtros que se utilizan para eliminarlo \cite{MandarMeghana, PawanManojSumitAshok}.

\subsection{Ruido}

Podemos considerar que una imagen degradada puede ser modelada de la siguiente manera:
$$g(x, y) = f(x, y) - \eta(x, y),$$
donde $f(x, y)$ es el píxel de la imagen original, $\eta(x, y)$ es el término de ruido, el cuál se considera que es una señal esporádica y aleatoria y $g(x, y)$ es el píxel degradado resultante \cite{MandarMeghana}.

\subsection{Modelos de Ruido}

El ruido produce efectos no deseados, para reducir los mismos, el aprendizaje previo de los modelos de ruido es esencial para un procesamiento posterior. El ruido digital puede surgir de varios tipos de fuentes, como los Dispositivos de Acoplamiento de Carga (CCD) y los sensores de Semiconducción de Óxido de Metal Complementario (CMOS). En cierto sentido, la función de dispersión de puntos (PSF) y la función de transferencia de modulación (MTF) se han utilizado para el análisis oportuno, completo y cuantitativo de los modelos de ruido. La función de densidad de probabilidad (PDF) o Histograma también se utiliza para diseñar y caracterizar los modelos de ruido \cite{AjayBrijendra,Dougherty}.\\

\textbf{Ruido Uniforme.}
El ruido uniforme es causado por la cuantificación de los píxeles de la imagen a un número de niveles distintos y se conoce como ruido de cuantificación. El nivel de los valores grises se distribuye uniformemente en un rango específico (ver Fig. \ref{fig:PDFUniformNoise}) y se puede utilizar para generar cualquier tipo diferente de distribución. Este ruido proporciona el ruido más neutral o imparcial y se usa a menudo para degradar imágenes para la evaluación de algoritmos de restauración de imágenes \cite{HaidiTheamSin}.
$$
p(z) = \left\{
    \begin{array}{ll}
        \frac{1}{(b-a)} & \mbox{if } a \leq z \leq b \\
        0  & \mbox{en otro caso}
    \end{array}
    \right.
$$
La media y la varianza de esta densidad están dadas por $\mu = \frac{(a+b)}{2}$ y $\sigma^2 = \frac{(b-a)^2}{12}$ \cite{MandarMeghana}.\\
    
\begin{figure}[h!]
    \centering
    \includegraphics[width=5cm]{Graphics/PDF-of-Uniform-Noise.png}
    \caption{PDF de ruido uniforme.}
    \label{fig:PDFUniformNoise}
\end{figure}

\textbf{Ruido Gaussiano.}
El ruido puede clasificarse como ruido substitutivo (ruido impulsivo (ruido de sal y pimienta), ruido impulsivo aleatorio, etc.) y ruido aditivo (por ejemplo, ruido gaussiano blanco aditivo). En muchas ocasiones el ruido en las imágenes digitales es aditivo por naturaleza con una potencia uniforme en todo el ancho de banda con distribución de probabilidad gaussiana. Este ruido se denomina ruido gaussiano blanco aditivo (RGBA) \cite{MandarMeghana}. El ruido gaussiano es causado por fuentes naturales como la vibración térmica de los átomos y la naturaleza discreta de la radiación de los objetos cálidos \cite{AjayBrijendra2}. Este modelo de ruido generalmente altera los valores grises en las imágenes digitales. Es por eso que está esencialmente diseñado y caracterizado por su función de densidad de probabilidad (PDF) o histograma normalizado (ver Fig. \ref{fig:PDFGaussianNoise}) con respecto al valor de gris \cite{AjayBrijendra}. La \textit{función de distribución de probabilidad} $p$ de una variable aleatoria Gaussiana $z$, está dada por:
$$p(z) = \frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{(z-\bar{z})^2}{2\sigma^2}},$$
donde $z$ representa el nivel de gris, $\bar{z}$ el valor medio y $\sigma$ la desviación estándar, donde $\sigma^2$ es la varianza.

\begin{figure}[h!]
    \centering
    \includegraphics[width=5cm]{Graphics/PDF-of-Gaussian-Noise.png}
    \caption{PDF de ruido gaussiano.}
    \label{fig:PDFGaussianNoise}
\end{figure}

\subsection{Filtros} \label{sec:NoiseFilters}

El propósito del filtrado de imágenes es reducir el ruido y mejorar la calidad visual de la imagen. Hay muchas maneras de eliminar el ruido de una imagen y se utilizan una variedad de algoritmos \cite{MandarMeghana, PawanManojSumitAshok} para ello. La propiedad más importante de un buen modelo de eliminación de ruidos en imágenes, es que debe eliminar completamente el ruido tanto como sea posible, así como preservar los bordes \cite{PawanManojSumitAshok}. El filtrado de imágenes hace posible varias tareas útiles en el procesamiento de imágenes. Se puede aplicar un filtro para reducir la cantidad de ruido no deseado en una imagen (ver Fig. \ref{fig:filters}) o para revertir los efectos de desenfoque en la misma \cite{RuchikaGaurav}. Existen principalmente dos tipos de algoritmos, lineales y no lineales. Los beneficios de los modelos de eliminación de ruido lineales es la velocidad y sus limitaciones, consisten, en que no son capaces de preservar los bordes de las imágenes de manera eficiente, es decir, los bordes, que se reconocen como discontinuidades en la imagen, se corrigen \cite{PawanManojSumitAshok}. Los filtros no lineales tienen un comportamiento bastante diferente en comparación con los filtros lineales, puesto que pueden manejar los bordes de una manera mucho mejor que los lineales además pueden producir resultados que varían de una manera no intuitiva \cite{RuchikaGaurav}.\\

\textbf{Promedio}
El filtrado medio o promedio es un método simple, intuitivo y fácil de implementar para suavizar imágenes \cite{RajeshUday}. Este filtro lineal es pobre para mantener los bordes dentro de la imagen \cite{JamesYixinStephen}. Se utiliza mayormente en la eliminación de ruido de grano \cite{PawanManojSumitAshok}, dado que tiene el efecto de eliminar los valores de píxeles que no son representativos de su entorno \cite{RajeshUday}. La idea del filtrado promedio es simplemente reemplazar cada valor de píxel en una imagen con el valor promedio de sus vecinos, incluido él mismo \cite{RajeshUday}. La forma y el tamaño del vecindario es representado por una ventana sobre cada píxel \cite{PawanManojSumitAshok}. La ventana suele ser cuadrada pero puede tener cualquier forma \cite{MandarMeghana}. A menudo se usa una ventana cuadrada de $3 x 3$, aunque se pueden usar ventanas más grandes para un alisado más severo. Una ventana pequeña se puede aplicar más de una vez para producir un efecto similar, pero no idéntico, como una sola pasada con una ventana grande\cite{RajeshUday}.\\

\textbf{Mediana}
El filtro de mediana es una técnica de filtrado digital no lineal, el cuál se basa en la clasificación de los valores de píxel contenidos en la región del filtro \cite{MandarMeghana}. Este es un filtro robusto y bastante popular para reducir ciertos tipos de ruido, es muy utilizado en el procesamiento digital de imágenes \cite{RuchikaGaurav}, puesto que bajo ciertas condiciones preserva los bordes de las imágenes mientras elimina el ruido. La idea principal es ir reemplazando cada pixel con la mediana de los pixeles vecinos en una ventana. Si la ventana tiene un número impar de entradas, entonces la mediana es simple de definir: \textit{valor medio de todas las entradas en la ventana ordenadas numéricamente}. Para un número par de entradas se utiliza para reemplazar el promedio de los dos valores de los píxeles del medio \cite{PawanManojSumitAshok}. Una gran ventaja de este filtro sobre filtros lineales es que puede eliminar el efecto de los valores de ruido de entrada con magnitudes extremadamente grandes \cite{MandarMeghana}, en cambio los filtros lineales son sensibles a este tipo de ruido, es decir, la salida puede degradarse severamente incluso por una pequeña fracción de los valores anómalos de ruido \cite{JamesYixinStephen, PawanManojSumitAshok}.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
	    \includegraphics[width=\linewidth]{Graphics/Filter-Original.png}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
	    \includegraphics[width=\linewidth]{Graphics/Mean-Filter-Filtered.png}
        \caption{}
    \end{subfigure}    
    \begin{subfigure}[b]{0.32\linewidth}
        \centering
	    \includegraphics[width=\linewidth]{Graphics/Median-Filter-Filtered.png}
        \caption{}
    \end{subfigure}    
    \caption{Filtros : (a) Imagen Original, (b) Filtro Promedio, (c) Filtro Mediana.}
    \label{fig:filters}
\end{figure}    

\section{Background Subtraction}\label{sec:BS}

Background Subtraction (BS) es una de las tareas más frecuentes en los sistemas de seguimiento y análisis de video. Es el primer paso para todo tipo de aplicaciones en el campo de visión por computadora, como análisis de video, seguimiento de objetos, videovigilancia, recuento de objetos, análisis de tráfico, etc \cite{GuangleTaoJiandanPingWenwu}. Por esta razón, generalmente se requiere que sea lo más rápido y simple posible. La popularidad de los algoritmos de background subtraction proviene en gran medida de su eficiencia computacional, permitiendo que aplicaciones como las antes mencionadas cumplan con sus objetivos en tiempo real \cite{DeepjoySarat}.

La idea básica en el enfoque, es detectar los objetos en movimiento a partir de la diferencia entre el cuadro actual y un cuadro de referencia, que se denomina imagen de fondo o modelo de fondo \cite{DeepjoySarat}. Esta solución ha demostrado ser exitosa siempre que la cámara esté rigurosamente estática con un fondo fijo libre de ruido \cite{YannickPierreMarcBrunoHeleneChristophe}. La imagen de fondo debe ser lo suficientemente buena para representar la escena y actualizarse periódicamente para que se adapte a las diferentes condiciones de luminancia y configuraciones de geometría. Una imagen de fondo deficiente puede dar lugar a resultados pobres de background subtraction, ya que se restará con la imagen actual para obtener el resultado final \cite{DeepjoySarat}.

Algunos videos con una pobre relación señal-ruido causada por una cámara de baja calidad, artefactos de compresión o un entorno ruidoso, es probable que generen numerosos falsos positivos. Los falsos positivos también pueden ser inducidos por cambios en la iluminación (gradual o repentino), un fondo animado (ondas en el agua, árboles sacudidos por el viento) o inestabilidad de la cámara por nombrar algunos \cite{YannickPierreMarcBrunoHeleneChristophe}.

Desde la década de 1990, se ha propuesto una gran cantidad de algoritmos BS y se han lanzado diferentes tipos de conjuntos de datos y puntos de referencia para evaluar dichos algoritmos \cite{GuangleTaoJiandanPingWenwu}.

\subsection{Algoritmos de Background Subtraction}

Muchos algoritmos de BS han sido diseñados para segmentar los objetos de primer plano del fondo de una secuencia (ver Fig. \ref{fig:BSExample}), y generalmente comparten el mismo esquema (ver Fig. \ref{fig:BSParadigm}).

\begin{figure}[h!]
	\centering
	\begin{subfigure}[b]{0.32\linewidth}
		\centering
		\includegraphics[width=\linewidth]{Graphics/BSExampleBackground.png}
		\caption{}
	\end{subfigure}
	\begin{subfigure}[b]{0.32\linewidth}
		\centering
		\includegraphics[width=\linewidth]{Graphics/BSExampleCurrentFrame.png}
		\caption{}
	\end{subfigure}
	\begin{subfigure}[b]{0.32\linewidth}
		\centering
		\includegraphics[width=\linewidth]{Graphics/BSExampleDetected.png}
		\caption{}
	\end{subfigure}
	\caption{Ejemplo de Background Subtraction : (a) Fondo, (b) Frame Actual, (c) Fondo Detectado.}
	\label{fig:BSExample}
\end{figure}

Este esquema esta dado debido a que la mayoría de las técnicas de BS comparten un denominador común: \textit{suponen que la secuencia de video observada $I$ está hecha de un fondo estático $B$ frente a los objetos en movimiento que se observan} \cite{YannickPierreMarcBrunoHeleneChristophe}. Con la suposición de que cada objeto en movimiento está hecho de un color o una distribución de color diferente del observado en $B$, numerosos métodos de BS se pueden resumir con la siguiente fórmula:
\begin{equation}
X_t(s) = \left\{
\begin{array}{ll}
	1 & \mbox{if } d(I_{s,t}, B_s) > \tau \\
	0               & \mbox{en otro caso}        
\end{array}
\right.
\label{eq:BSFormula}
\end{equation}
donde $\tau$ es un umbral, $X_t$ es el campo de etiqueta de movimiento en el tiempo $t$ (también llamado máscara de movimiento), $d$ es la distancia entre $I_{s,t}$ color en el tiempo $t$ y el píxel $s$ y $B_s$ modelo de fondo en el pixel $s$. La diferencia principal entre varios métodos BS es cómo se modela $B$ y qué medida de distancia $d$ usan.\\

\begin{figure}[h!]
	\centering
	\includegraphics[width=11cm]{Graphics/BSParadigm.png}
	\caption{Paradigma de Background Subtraction.}
	\label{fig:BSParadigm}
\end{figure}

\textbf{Diferencia de Fotograma.}
La diferencia de fotograma es la forma más simple de background subtraction \cite{DeepjoySarat}. El fotograma actual simplemente se resta del fotograma anterior, y si la diferencia en los valores de píxel para un píxel dado es mayor que un umbral $\tau$, entonces el píxel se considera parte del primer plano. Dada la equación \ref{eq:BSFormula} podemos formular el método de la siguiente forma:
$$
X_t(s) = \left\{
\begin{array}{ll}
	1 & \mbox{if } \mid I_{s,t} - I_{s,t-1} \mid > \tau \\
	0 & \mbox{en otro caso}
\end{array}
\right.
$$
donde el modelo $B_s$ se define como $I_{s,t-1}$ y la medida de distancia $d$ está dada por $\mid I_{s,t} - I_{s,t-1} \mid$.

Este método no necesita de un historial de fotogramas, ni de realizar varios cálculos, lo cuál presupone una ventaja ya que no consume tiempo ni memoria \cite{InsafSlimane}, otra ventaja es que se adapta a los cambios en el fondo más rápido que cualquier otro método, puesto que, el fondo es solo el fotograma anterior. Sin embargo, como no se conoce el historial de fotogramas, no se puede mantener el fondo multimodal y es imposible evitar la detección de objetos espurios, como el balanceo de las ramas de los árboles, lo cuál también se conoce como el problema de apertura descrito en \cite{ApewokinApewokinWillsWillsGentile} y \cite{KentaroJohnBarryBrian}.\\

\textbf{Gaussiano Simple.}
Modelar $B$ con una sola imagen como el método \textit{Diferencia de Fotograma} requiere un fondo rigurosamente fijo y libre de ruido. Como este requisito no se puede satisfacer en todos los escenarios de la vida real, muchos algoritmos modelan cada píxel de fondo con una función de densidad de probabilidad (PDF) aprendida sobre una serie de marcos de entrenamiento \cite{YannickPierreMarcBrunoHeleneChristophe}. En este caso, el problema de BS se convierte en un problema de umbral de PDF para el cual es probable que un píxel con baja probabilidad corresponda a un objeto en movimiento en primer plano. Por ejemplo, para tener en cuenta el ruido, \cite{WrenAzarbayejaniDarrellPentland} modela cada píxel de fondo con una distribución gaussiana $\Gamma(\mu_{s,t}, \Sigma_{s,t})$ donde  $\mu_{s,t}$ y $\Sigma_{s,t}$ representan respectivamente el color de fondo promedio y la matriz de covarianza en el píxel $s$ y tiempo $t$. En este contexto, la métrica de distancia puede ser la siguiente probabilidad de registro \cite{YannickPierreMarcBrunoHeleneChristophe}:

$$d_G = \frac{1}{2}\log((2\pi)^3\mid\Sigma_{s,t}\mid)+\frac{1}{2}(I_{s,t}-\mu_{s,t})\Sigma_{s,t}^{-1}(I_{s,t}-\mu_{s,t})^T$$
o una distancia Mahalanobis:

$$d_M = \mid I_{s,t}-\mu_{s,t}\mid\Sigma_{s,t}^{-1}\mid I_{s,t}-\mu_{s,t}\mid^T$$

Los valores de píxeles que difieren más de una constante multiplicada por la desviación estándar de su media se consideran parte del primer plano. Como la iluminación a menudo cambia con el tiempo, la media y la covarianza de cada píxel también se pueden actualizar iterativamente utilizando una constante de actualización \cite{YannickPierreMarcBrunoHeleneChristophe}:
\begin{gather*}
    \begin{split}
        \mu_{s,t+1} & = (1-\alpha)\mu_{s,t}+\alpha I_{s,t}\\
        \Sigma_{s,t+1} & = (1-\alpha)\Sigma_{s,t}+\alpha(I_{s,t}-\mu_{s,t})(I_{s,t}-\mu_{s,t})^T
    \end{split}
\end{gather*}

Las ventajas y desventajas del método se pueden ver en \cite{BenezethJodoinEmileLaurentRosenberger} y \cite{GreffBrandoKrauStrickerClua}. En \cite{BenezethJodoinEmileLaurentRosenberger}, el \textit{Gaussiano ponderado por una matriz de covarianza} compensó las inestabilidades de fondo y en \cite{GreffBrandoKrauStrickerClua} el \textit{Gaussiano Simple} se vió afectado por las altas varianzas de píxeles alternados y la baja varianza de píxeles estables.

\subsection{Métricas de Evaluación}

Background subtraction se considera un problema de clasificación binaria: \textit {un píxel se etiqueta como fondo o primer plano} \cite{GuangleTaoJiandanPingWenwu}. Como se muestra en la Figura \ref{fig:BSMetrics}, el círculo y el cuadrado respectivamente representan el primer plano verdadero y el detectado. TP es el número de verdaderos positivos, TN es el número de negativos verdaderos, FN es el número de falsos negativos y FP es el número de falsos positivos.\\

\begin{figure}[!h]
    \centering
    \includegraphics[width=8cm]{Graphics/BSMetrics.png}
    \caption{Primer Plano verdadero y detectado en BS.}
    \label{fig:BSMetrics}
\end{figure}

Tres importantes métricas de evaluación se calculan con las siguientes ecuaciones:

\begin{equation*}
    \begin{split}
        \text{Precisión} & = TP/(TP + FP)\\
        Recobrado & = TP/(TP + FN)\\
        Medida-F & =  2 * Recobrado * \text{Precisión}/(Recobrado + \text{Precisión})
    \end{split}
\end{equation*}

La precisión se puede ver como una métrica de exactitud o calidad, mientras que la recuperación es una medida de integridad o cantidad. Para un mejor algoritmo de BS, los puntajes de precisión y recuperación deben ser altos, pero existe una relación inversa entre estos, donde es posible aumentar uno al costo de reducir al otro \cite{GuangleTaoJiandanPingWenwu}. Como un umbral $\tau$ para un método produce una sola pareja de precisión/recuperación por video, se usan 15 umbrales diferentes para producir curvas. De esta manera, la comparación entre métodos se hace más fácil ya que no tenemos que encontrar el mejor umbral para cada método sobre cada video \cite{YannickPierreMarcBrunoHeleneChristophe}. La \textit{Medida-F}, que es un medio armónico de precisión y recuperación, puede verse como un compromiso entre ambas. Equilibra la precisión y la recuperación con pesos iguales, y solo es alta cuando tanto la recuperación como la precisión son altas. Una puntuación más alta de \textit{Medida-F} significa que el rendimiento del algoritmo BS es mejor \cite{GuangleTaoJiandanPingWenwu}.

\section{RANSAC} \label{sec:RANSAC}

Las técnicas generales de estimación de parámetros se basan en la suposición de que el ruido en los datos de entrenamiento es ruido gaussiano con media cero. Por lo tanto, este ruido se suavizaría después de la optimización de parámetros que promedia todos los datos. Sin embargo, esta suposición no se cumple cuando existen errores graves que pueden ocurrir con valores extremos de ruido o con mediciones incorrectas durante el muestreo. Este tipo de errores en realidad no se ajustan al modelo y deben rechazarse durante el entrenamiento para obtener mejores ajustes del modelo.

\begin{figure}[b]
    \centering
    \includegraphics[width=10cm]{Graphics/RANSACGraphic.png}
    \caption{Ejemplo del algoritmo \textit{RANSAC}.}
    \label{fig:RANSACExample}
\end{figure}

El algoritmo \textit{RANSAC} (\textit{RANdom SAmple Consensus}) propuesto por \cite{FischlerBolles} es un algoritmo iterativo utilizado para estimar los parámetros de un modelo matemático de un conjunto de datos que contiene \textit{outliers} \cite{PabloJuan}. Otros métodos para estimar parámetros de modelos como mínimos cuadrados le dan el mismo peso a todos los datos, por lo que la presencia de un \textit{outlier} puede llegar a distorsionar el modelo obtenido (Ver Fig. \ref{fig:RANSACExample}). Este algoritmo utiliza una técnica de remuestreo que genera soluciones candidatas mediante el uso del número mínimo de observaciones (puntos de datos) necesarias para estimar los parámetros del modelo subyacente \cite{Konstantinos}.

\subsection{Algoritmo}

Para entender mejor lo que \textit{RANSAC} hace, se describe el flujo del algoritmo genéricamente de la siguiente manera:

\begin{figure}[!h]
	\centering
	\includegraphics[width=\linewidth]{Graphics/RANSACAlgorithm1.png}
	\caption{Algoritmo \textit{RANSAC}.}
	\label{fig:RANSACAlgorithm}
\end{figure}

% \begin{algorithm}[!h]
% 	\SetAlgoLined
% 	\KwData{data - a set of observed data points\\
%             model - a model that can be fitted to data points\\
%             n - the minimum number of data values required to fit the model
%             k - the maximum number of iterations allowed in the algorithm
%             t - a threshold value for determining when a data point fits a model
%             d - the number of close data values required to assert that a model fits well to data}
% 	\KwResult{bestfit - model parameters which best fit the data (or nil if no good model is found)}
%     \everypar={\nl}
%     iterations = 0\;
%     bestfit = nil\;
%     besterr = something really large\;
%     \While{$iterations < k$}{
% 	maybeinliers = n randomly selected values from data\;
% 	maybemodel = model parameters fitted to maybeinliers\;
%     alsoinliers = empty set\;
% 	\For {every point in data not in maybeinliers} {
% 		\If{point fits maybemodel with an error smaller than t}
% 		    {add point to alsoinliers\;}
% 	}
% 	\If{the number of elements in alsoinliers is > d} {
% 		% this implies that we may have found a good model
% 		% now test how good it is
% 		bettermodel = model parameters fitted to all points in maybeinliers and alsoinliers\;
% 		thiserr = a measure of how well model fits these points\;
% 		\If {thiserr < besterr} {
% 			bestfit = bettermodel\;
% 			besterr = thiserr\;
% 		}
% 	}
% 	increment iterations\;
% }
% return bestfit\;
%     \caption{RANSAC.}
%     \label{alg:RANSAC}    
% \end{algorithm}

Como se describe al principio del flujo del algoritmo, \textit{RANSAC} necesita algunos parámetros predefinidos para ejecutarse: tamaño del subconjunto de muestra ($n$), umbral de tolerancia de error ($t$), umbral de consenso mínimo ($d$) y número de iteraciones ($k$).

\subsection{Parámetros}

Como \textit{RANSAC} es un algoritmo aleatorizado, es esencial estimar verdaderamente los parámetros, a fin de aumentar la probabilidad de encontrar el modelo óptimo, manteniendo las ventajas computacionales de un algoritmo aleatorizado frente a un algoritmo determinista exhaustivo.\\

\textbf{Tamaño del Subconjunto de Muestra.}
El tamaño del subconjunto de muestra es el número de muestras que \textit{RANSAC} elige para ajustar el modelo en cada iteración. \textit{RANSAC} usa la cantidad mínima de muestras necesarias para definir el modelo. Por ejemplo, para ajustarse a un modelo lineal, elige 2 muestras de datos o para ajustarse a un modelo cuadrático, selecciona 3 muestras de datos.

$$n = \text{número mínimo de muestras para definir el modelo}$$

Se podría pensar que al usar más muestras de datos, el subconjunto mínimo sería ventajoso, ya que se puede obtener inicialmente una estimación mejor y más precisa del modelo. Sin embargo, tener más muestras en el subconjunto de muestra aumentaría el espacio de búsqueda para la selección del subconjunto. Entonces, para mantener la probabilidad de encontrar el modelo óptimo en el mismo nivel, deberíamos probar más subconjuntos de muestra. Por lo tanto, se requiere un aumento en el número de iteraciones, que en su mayoría provoca un aumento en la complejidad computacional que supera las ventajas de tener un subconjunto de muestra más grande.\\

\textbf{Umbral de Tolerancia de Error.}
El umbral de tolerancia de error ($t$) es utilizado por \textit{RANSAC} para determinar si una muestra de datos concuerda con un modelo o no. Nos gustaría elegir este umbral, tal que con una probabilidad $\alpha$ el punto sea un \textit{inlier}. Este cálculo requiere la distribución de probabilidad para la distancia de un \textit{inlier} del modelo. En la práctica, el umbral generalmente se elige empíricamente, sin embargo, si se supone que el error de medición es gaussiano con media cero y desviación estándar $\sigma$, entonces se puede calcular un valor para $t$ \cite{RichardAndrew}. En este caso, el cuadrado de la distancia del punto ($d^2$), es una suma de variables gaussianas al cuadrado y sigue una distribución de $\chi_{m}^2$ con $m$ grados de libertad, donde $m$ es igual a la codificación del modelo. Para una línea, la codificación es $1$, solo se mide la distancia perpendicular a la línea. Si el modelo es un punto, la codificación es $2$, y el cuadrado de la distancia es la suma de los errores de medición $x$ e $y$ al cuadrado \cite{RichardAndrew}.

La probabilidad de que el valor de una variable aleatoria de $\chi_{m}^2$ sea menor que $k^2$ viene dada por la distribución acumulada de chi-cuadrado, $F_m(k^2) = \int_{0}^{k^2} \chi_{m}^2(\xi)d\xi$ \cite{RichardAndrew}.

$$
\left\{
\begin{array}{ll}
	inlier & d^2 < t^2\\
	outlier & d^2 \geq t^2
\end{array}
\right.
\mbox{ with } t^2 = F_{m}^{-1}(\alpha)\sigma^2
$$

Por lo general, $\alpha$ se elige como $0,95$, por lo que hay un 95\% de probabilidad de que el punto sea un \textit{inlier}. Esto significa que un \textit{inlier} solo será rechazado incorrectamente el 5\% del tiempo \cite{RichardAndrew}.\\

\textbf{Umbral de Consenso Mínimo.}
El umbral de consenso mínimo es el número mínimo de muestras que se aceptarían como un consenso válido para generar un modelo final para esa iteración. Una regla empírica es terminar si el tamaño del conjunto de consenso es similar al número de \textit{inliers} que se cree que están en el conjunto de datos dada la proporción supuesta de \textit{outliers}, es decir, para $n$ puntos de datos $d = (1 - \epsilon)n$ \cite{RichardAndrew}.\\

\textbf{Número de Iteraciones.}
Los algoritmos deterministas exhaustivos probarían todos los subconjuntos de muestras posibles para encontrar el mejor, pero en realidad no solo es computacionalmente inviable, sino también innecesario. Por lo tanto, en lugar de una forma determinista, \textit{RANSAC} elige un subconjunto de muestras al azar. Pero, también es importante determinar el número de estas elecciones aleatorias para obtener una alta probabilidad de que \textit{RANSAC} elija un subconjunto de muestra que no incluya \textit{outliers}.
El número de iteraciones $k$ se calcula en función de la probabilidad ($p$) de que por lo menos un subconjunto que se elija aleatoriamente este libre de \textit{outliers}. $\omega$ es la probabilidad de que un punto dentro del conjunto de datos sea un \textit{inlier}, por lo tanto $\epsilon = 1-\omega$ es la probabilidad de encontrar un \textit{outlier} \cite{PabloJuan}. Para que se cumpla la probabilidad $p$ se tiene que cumplir:

$$1 - p = (1 - u^m)^k$$
despejando $k$ obtenemos:
$$k = \frac{\log(1 - p)}{\log(1 - (1 - \epsilon)^s)}$$
